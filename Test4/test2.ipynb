{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2b2047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DSAI\\ESG\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import tabula\n",
    "import faiss\n",
    "import json\n",
    "import base64\n",
    "import pymupdf\n",
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import pytesseract\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_aws import ChatBedrock\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import re\n",
    "from pathlib import Path\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a66757a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-24'  # Replace XX with your version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d4d3100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Financial PDF Data Extractor with RAG ===\n",
      "Specialized for precise financial data extraction from PDFs\n",
      "\n",
      "=== Main Menu ===\n",
      "1. Process new PDF\n",
      "2. Query current PDF\n",
      "3. Exit\n",
      "\n",
      "=== PDF Source Selection ===\n",
      "1. Download from URL\n",
      "2. Use local file\n",
      "3. Select from common financial document URLs\n",
      "✓ Selected: D:\\DSAI\\ESG\\ESG-SCG_documents\\EconomicPerformance2023.pdf\n",
      "\n",
      "=== Processing PDF: EconomicPerformance2023.pdf ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages: 100%|██████████| 2/2 [00:04<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating Embeddings ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s] ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 105.80it/s]06,  1.68it/s, Text: 0/9, Table: 1/1, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 100.69it/s]06,  1.68it/s, Text: 1/9, Table: 1/1, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 101.85it/s]05,  1.68it/s, Text: 2/9, Table: 1/1, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.15it/s]:05,  1.68it/s, Text: 3/9, Table: 1/1, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.69it/s]:04,  1.68it/s, Text: 4/9, Table: 1/1, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 105.55it/s]04,  1.68it/s, Text: 5/9, Table: 1/1, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]:03,  1.68it/s, Text: 6/9, Table: 1/1, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 124.30it/s]00, 10.62it/s, Text: 7/9, Table: 1/1, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 110.24it/s]00, 10.62it/s, Text: 8/9, Table: 1/1, Image: 0/0]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:00<00:00, 12.86it/s, Text: 9/9, Table: 1/1, Image: 0/0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created FAISS index with 10 embeddings\n",
      "✓ Processed 12 items from 2 pages\n",
      "\n",
      "=== Processing Summary ===\n",
      "Table: 1\n",
      "Financial_text: 9\n",
      "Page: 2\n",
      "\n",
      "✓ Successfully processed: D:\\DSAI\\ESG\\ESG-SCG_documents\\EconomicPerformance2023.pdf\n",
      "  - 12 items extracted\n",
      "  - 2 pages processed\n",
      "\n",
      "=== Main Menu ===\n",
      "1. Process new PDF\n",
      "2. Query current PDF\n",
      "3. Exit\n",
      "\n",
      "=== Querying: what is the revenue of 2021 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.95it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2385 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found 15 relevant items\n",
      "\n",
      "=== Response ===\n",
      "Answer: 2021 EBITDA (Billion Baht) 32.0 34.1 47.2 21.4 25.9 GRI 201-1 Profit for the year (Billion Baht) 32.0 34.1 47.2 21.4 25.9\n",
      "\n",
      "    Confidence: High (100.0%)\n",
      "    Source pages: 0, 0, 0\n",
      "\n",
      "    Additional context: Page 0: Performance Data 2019 2020 2021 2022 2023 GRI Standards Unnamed: 0 SASB --------------------------------------------------------------------------------------- Revenue from sales (Billion Baht) 438.0 399.9 530.1 569.6 499.6 GRI 201-1 Profit for the year (Billion Baht) 32.0 34.1 47.2 21.4 25.9 GRI 201-1 EBITDA (Billion Baht) 75.1 74.6 91.9 61.9 54.1 GRI 201-1 Employee compensation comprising salary, wage, welfare, 48,139 46,796 47,921 50,732 50,190 GRI 201-1 and regular contributions (Million Baht) Dividend to shareholders (Million Baht) 16,800 16,800 22,200 9,600 7,200 GRI 201-1 Interest and financial expenses to lender (Million Baht) 6,442 7,082 6,758 7,523 10,297 GRI 201-1 Taxes to government and local government authorities such as income tax, local maintenance tax, property tax and 6,143 7,190 8,430 6,685 6,153 GRI 201-1 other specific taxes (Million Baht) Tax privilege and others from investment promotion, 1,388 1,149 1,829 1,054 1,248 GRI 201-4 and research and development (Million Baht) Non-compliance case through SCG Whistleblowing System (Cases) 30 38 30 51 55 GRI 205-3 Customer Satisfaction - SCG Contact Center (%) 100 100 100 100 100 Average Customer Satisfaction - All business unit (%) 94 94 94 94 94 Contributions to organizations (Million Baht)(1) 22.2 13.79 11.31 30.9 27.8 Contributions to political activities (Million Baht)(2) 0 0 0 0 0 Suppliers that assessed Environmental, Social and Governance (ESG) 100 100 100 100 100 Risks (% of procurement spending) Procurement Spending by Geography (% of procurement spending) • Domestic 58 57 40 50 55 • Regional 42 43 60 50 45 Revenue from Sales of High Value Added Products and Services (Billion Baht) 179.2 126.1 182.7 195.5 167.7 (%) 40.9 31.5 34.5 34.3 33.6 Revenue from Sales of SCG Green Choice Products and Services (Billion Baht) 128.8 130.4 216.0 289.7 270.7 EM-CM-410a.2 (%) 29.4 32.6 40.7 50.9 54.1 Revenue from Sales of Products and Services designed for use-phase resource efficiency (Billion Baht)(3) NA 0.022 4.870 27.46 71.5 RT-CH-410a.1 (%) NA 0.02 2.00 11.6 14.3 Revenue from Sales of Sustainable Construction Products and Services (Billion Baht) 60.4 59.6 69.4 71.8 59.3 EM-CM-410a.1 (%) 13.8 14.9 13.1 12.6 31.3\n",
      "\n",
      "=== Main Menu ===\n",
      "1. Process new PDF\n",
      "2. Query current PDF\n",
      "3. Exit\n",
      "\n",
      "=== Querying: revenue of the sales for year 2021 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 98.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found 15 relevant items\n",
      "\n",
      "=== Response ===\n",
      "Answer: (Billion Baht) 438.0 399.9 530.1 569.6 499.6 GRI 201-1 Profit for the year (Billion Baht) 32.0 34.1 47.2 21.4 25.9\n",
      "\n",
      "    Confidence: High (100.0%)\n",
      "    Source pages: 0, 0, 0\n",
      "\n",
      "    Additional context: Page 0: Performance Data 2019 2020 2021 2022 2023 GRI Standards Unnamed: 0 SASB --------------------------------------------------------------------------------------- Revenue from sales (Billion Baht) 438.0 399.9 530.1 569.6 499.6 GRI 201-1 Profit for the year (Billion Baht) 32.0 34.1 47.2 21.4 25.9 GRI 201-1 EBITDA (Billion Baht) 75.1 74.6 91.9 61.9 54.1 GRI 201-1 Employee compensation comprising salary, wage, welfare, 48,139 46,796 47,921 50,732 50,190 GRI 201-1 and regular contributions (Million Baht) Dividend to shareholders (Million Baht) 16,800 16,800 22,200 9,600 7,200 GRI 201-1 Interest and financial expenses to lender (Million Baht) 6,442 7,082 6,758 7,523 10,297 GRI 201-1 Taxes to government and local government authorities such as income tax, local maintenance tax, property tax and 6,143 7,190 8,430 6,685 6,153 GRI 201-1 other specific taxes (Million Baht) Tax privilege and others from investment promotion, 1,388 1,149 1,829 1,054 1,248 GRI 201-4 and research and development (Million Baht) Non-compliance case through SCG Whistleblowing System (Cases) 30 38 30 51 55 GRI 205-3 Customer Satisfaction - SCG Contact Center (%) 100 100 100 100 100 Average Customer Satisfaction - All business unit (%) 94 94 94 94 94 Contributions to organizations (Million Baht)(1) 22.2 13.79 11.31 30.9 27.8 Contributions to political activities (Million Baht)(2) 0 0 0 0 0 Suppliers that assessed Environmental, Social and Governance (ESG) 100 100 100 100 100 Risks (% of procurement spending) Procurement Spending by Geography (% of procurement spending) • Domestic 58 57 40 50 55 • Regional 42 43 60 50 45 Revenue from Sales of High Value Added Products and Services (Billion Baht) 179.2 126.1 182.7 195.5 167.7 (%) 40.9 31.5 34.5 34.3 33.6 Revenue from Sales of SCG Green Choice Products and Services (Billion Baht) 128.8 130.4 216.0 289.7 270.7 EM-CM-410a.2 (%) 29.4 32.6 40.7 50.9 54.1 Revenue from Sales of Products and Services designed for use-phase resource efficiency (Billion Baht)(3) NA 0.022 4.870 27.46 71.5 RT-CH-410a.1 (%) NA 0.02 2.00 11.6 14.3 Revenue from Sales of Sustainable Construction Products and Services (Billion Baht) 60.4 59.6 69.4 71.8 59.3 EM-CM-410a.1 (%) 13.8 14.9 13.1 12.6 31.3\n",
      "\n",
      "=== Main Menu ===\n",
      "1. Process new PDF\n",
      "2. Query current PDF\n",
      "3. Exit\n",
      "\n",
      "=== Querying: Taxes to government and local government authorities ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 80.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found 15 relevant items\n",
      "\n",
      "=== Response ===\n",
      "Answer: GRI 201-1 Employee compensation comprising salary, wage, welfare, and regular contributions (Million Baht) 48,139 46,796 47,921 50,732 50,190 GRI 201-1 Dividend\n",
      "\n",
      "    Confidence: Medium (60.0%)\n",
      "    Source pages: 0, 0, 0\n",
      "\n",
      "    \n",
      "\n",
      "=== Main Menu ===\n",
      "1. Process new PDF\n",
      "2. Query current PDF\n",
      "3. Exit\n",
      "\n",
      "=== PDF Source Selection ===\n",
      "1. Download from URL\n",
      "2. Use local file\n",
      "3. Select from common financial document URLs\n",
      "✓ Selected: D:\\DSAI\\ESG\\ESG-SCG_documents\\SocialPerformance2023.pdf\n",
      "\n",
      "=== Processing PDF: SocialPerformance2023.pdf ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages: 100%|██████████| 6/6 [00:04<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating Embeddings ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.92it/s] ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 58.34it/s] ?it/s, Text: 0/30, Table: 1/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 94.97it/s]:01, 25.17it/s, Text: 1/30, Table: 1/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 97.01it/s]:01, 34.75it/s, Text: 2/30, Table: 1/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.24it/s]:00, 40.81it/s, Text: 3/30, Table: 1/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 100.05it/s]00, 43.85it/s, Text: 4/30, Table: 1/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 94.14it/s]:00, 55.21it/s, Text: 5/30, Table: 1/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 94.92it/s]:00, 55.21it/s, Text: 5/30, Table: 2/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 98.15it/s]:00, 55.21it/s, Text: 6/30, Table: 2/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 98.93it/s]:00, 55.21it/s, Text: 7/30, Table: 2/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 104.50it/s]:00, 55.21it/s, Text: 8/30, Table: 2/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 94.64it/s]0:00, 55.21it/s, Text: 9/30, Table: 2/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.59it/s]0:00, 63.08it/s, Text: 10/30, Table: 2/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 107.96it/s]:00, 63.08it/s, Text: 10/30, Table: 3/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 98.27it/s]0:00, 63.08it/s, Text: 11/30, Table: 3/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 110.33it/s]:00, 63.08it/s, Text: 12/30, Table: 3/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 75.87it/s]0:00, 63.08it/s, Text: 13/30, Table: 3/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 107.91it/s]:00, 63.08it/s, Text: 13/30, Table: 4/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 100.04it/s]:00, 68.87it/s, Text: 14/30, Table: 4/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 111.10it/s]:00, 68.87it/s, Text: 15/30, Table: 4/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 98.19it/s]0:00, 68.87it/s, Text: 16/30, Table: 4/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 109.41it/s]:00, 68.87it/s, Text: 17/30, Table: 4/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.24it/s]0:00, 68.87it/s, Text: 18/30, Table: 4/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.75it/s]0:00, 68.87it/s, Text: 19/30, Table: 4/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.68it/s]0:00, 67.45it/s, Text: 20/30, Table: 4/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.76it/s]0:00, 67.45it/s, Text: 20/30, Table: 5/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.39it/s]0:00, 67.45it/s, Text: 21/30, Table: 5/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 97.75it/s]0:00, 67.45it/s, Text: 22/30, Table: 5/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 104.94it/s]:00, 67.45it/s, Text: 23/30, Table: 5/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 109.85it/s]:00, 67.45it/s, Text: 24/30, Table: 5/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.35it/s]0:00, 67.45it/s, Text: 25/30, Table: 5/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 107.27it/s]:00, 57.14it/s, Text: 26/30, Table: 5/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 58.74it/s]0:00, 57.14it/s, Text: 27/30, Table: 5/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 111.13it/s]:00, 57.14it/s, Text: 28/30, Table: 5/5, Image: 0/0]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.98it/s]0:00, 57.14it/s, Text: 29/30, Table: 5/5, Image: 0/0]\n",
      "Generating embeddings: 100%|██████████| 41/41 [00:00<00:00, 61.91it/s, Text: 30/30, Table: 5/5, Image: 0/0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created FAISS index with 35 embeddings\n",
      "✓ Processed 41 items from 6 pages\n",
      "\n",
      "=== Processing Summary ===\n",
      "Table: 5\n",
      "Financial_text: 30\n",
      "Page: 6\n",
      "\n",
      "✓ Successfully processed: D:\\DSAI\\ESG\\ESG-SCG_documents\\SocialPerformance2023.pdf\n",
      "  - 41 items extracted\n",
      "  - 6 pages processed\n",
      "\n",
      "=== Main Menu ===\n",
      "1. Process new PDF\n",
      "2. Query current PDF\n",
      "3. Exit\n",
      "\n",
      "=== Querying: hours worked by Employee of 2020 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 119.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found 15 relevant items\n",
      "\n",
      "=== Response ===\n",
      "Answer: 0 ------------------------------------------------------------------------ Occupational Illness Frequency Rate (Cases/1,000,000 Hours Worked) • Employee\n",
      "\n",
      "    Confidence: Medium (60.0%)\n",
      "    Source pages: 0, 0, 3\n",
      "\n",
      "    \n",
      "\n",
      "=== Main Menu ===\n",
      "1. Process new PDF\n",
      "2. Query current PDF\n",
      "3. Exit\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class FinancialPDFProcessor:\n",
    "    \"\"\"\n",
    "    A comprehensive PDF processor specialized for financial documents with multi-modal RAG capabilities.\n",
    "    Handles both text-based and image-based tables with high precision.\n",
    "    \"\"\"\n",
    "    \n",
    "    # def __init__(self, embedding_dimension: int = 384, chunk_size: int = 700, chunk_overlap: int = 200):\n",
    "    #     self.embedding_dimension = embedding_dimension\n",
    "    #     self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #         chunk_size=chunk_size, \n",
    "    #         chunk_overlap=chunk_overlap, \n",
    "    #         length_function=len\n",
    "    #     )\n",
    "    #     self.bedrock_client = boto3.client(service_name=\"bedrock-runtime\", region_name='us-east-1')\n",
    "    #     self.model_id = \"amazon.titan-embed-image-v1\"\n",
    "    #     self.llm_model_id = \"amazon.nova-pro-v1:0\"\n",
    "    #     self.index = None\n",
    "    #     self.items = []\n",
    "    #     self.current_pdf_info = {}\n",
    "    \n",
    "    def __init__(self, embedding_dimension: int = 384, chunk_size: int = 700, chunk_overlap: int = 200):\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap, \n",
    "            length_function=len\n",
    "        )\n",
    "        # Replace AWS with free models\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "        self.qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "        self.index = None\n",
    "        self.items = []\n",
    "        self.current_pdf_info = {}\n",
    "        \n",
    "    def select_pdf_source(self) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Interactive PDF selection - either from URL or local file.\n",
    "        Returns: (filepath, source_type)\n",
    "        \"\"\"\n",
    "        print(\"\\n=== PDF Source Selection ===\")\n",
    "        print(\"1. Download from URL\")\n",
    "        print(\"2. Use local file\")\n",
    "        print(\"3. Select from common financial document URLs\")\n",
    "        \n",
    "        choice = input(\"Choose option (1-3): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            return self._download_from_url()\n",
    "        elif choice == \"2\":\n",
    "            return self._select_local_file()\n",
    "        elif choice == \"3\":\n",
    "            return self._select_from_common_urls()\n",
    "        else:\n",
    "            print(\"Invalid choice. Using local file selection.\")\n",
    "            return self._select_local_file()\n",
    "    \n",
    "    def _download_from_url(self) -> Tuple[str, str]:\n",
    "        \"\"\"Download PDF from URL\"\"\"\n",
    "        url = input(\"Enter PDF URL: \").strip()\n",
    "        filename = input(\"Enter filename (with .pdf extension): \").strip()\n",
    "        \n",
    "        if not filename.endswith('.pdf'):\n",
    "            filename += '.pdf'\n",
    "            \n",
    "        filepath = os.path.join(\"data\", filename)\n",
    "        os.makedirs(\"data\", exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(filepath, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            \n",
    "            print(f\"✓ Downloaded: {filepath}\")\n",
    "            return filepath, \"url\"\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Download failed: {e}\")\n",
    "            return self._select_local_file()\n",
    "    \n",
    "    def _select_local_file(self) -> Tuple[str, str]:\n",
    "        \"\"\"Select local PDF file\"\"\"\n",
    "        filepath = input(\"Enter path to local PDF file: \").strip()\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"✗ File not found: {filepath}\")\n",
    "            # List available PDFs in current directory\n",
    "            pdf_files = [f for f in os.listdir('.') if f.endswith('.pdf')]\n",
    "            if pdf_files:\n",
    "                print(\"\\nAvailable PDF files in current directory:\")\n",
    "                for i, pdf in enumerate(pdf_files, 1):\n",
    "                    print(f\"{i}. {pdf}\")\n",
    "                try:\n",
    "                    choice = int(input(\"Select file number: \")) - 1\n",
    "                    filepath = pdf_files[choice]\n",
    "                except (ValueError, IndexError):\n",
    "                    raise FileNotFoundError(\"No valid PDF file selected\")\n",
    "            else:\n",
    "                raise FileNotFoundError(\"No PDF files found\")\n",
    "        \n",
    "        print(f\"✓ Selected: {filepath}\")\n",
    "        return filepath, \"local\"\n",
    "    \n",
    "    def _select_from_common_urls(self) -> Tuple[str, str]:\n",
    "        \"\"\"Select from predefined financial document URLs\"\"\"\n",
    "        common_urls = {\n",
    "            \"1\": (\"Sample Annual Report\", \"https://www.sec.gov/Archives/edgar/data/320193/000032019323000077/aapl-20230930.htm\"),\n",
    "            \"2\": (\"Financial Statement Sample\", \"https://arxiv.org/pdf/1706.03762.pdf\"),  # Placeholder\n",
    "            \"3\": (\"Custom URL\", \"\")\n",
    "        }\n",
    "        \n",
    "        print(\"\\nCommon Financial Documents:\")\n",
    "        for key, (name, url) in common_urls.items():\n",
    "            print(f\"{key}. {name}\")\n",
    "        \n",
    "        choice = input(\"Select option: \").strip()\n",
    "        \n",
    "        if choice == \"3\":\n",
    "            return self._download_from_url()\n",
    "        elif choice in common_urls:\n",
    "            name, url = common_urls[choice]\n",
    "            filename = f\"{name.lower().replace(' ', '_')}.pdf\"\n",
    "            filepath = os.path.join(\"data\", filename)\n",
    "            \n",
    "            # Download logic here\n",
    "            return self._download_specific_url(url, filepath)\n",
    "        else:\n",
    "            return self._download_from_url()\n",
    "    \n",
    "    def _download_specific_url(self, url: str, filepath: str) -> Tuple[str, str]:\n",
    "        \"\"\"Download from specific URL\"\"\"\n",
    "        os.makedirs(\"data\", exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(filepath, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            \n",
    "            print(f\"✓ Downloaded: {filepath}\")\n",
    "            return filepath, \"url\"\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Download failed: {e}\")\n",
    "            return self._select_local_file()\n",
    "    \n",
    "    def create_directories(self, base_dir: str):\n",
    "        \"\"\"Create necessary directories for processing\"\"\"\n",
    "        directories = [\"images\", \"text\", \"tables\", \"page_images\", \"extracted_tables\"]\n",
    "        for directory in directories:\n",
    "            os.makedirs(os.path.join(base_dir, directory), exist_ok=True)\n",
    "    \n",
    "    def extract_table_from_image(self, image_path: str, page_num: int) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract tabular data from image using OCR with enhanced preprocessing for financial data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                return None\n",
    "            \n",
    "            # Preprocessing for better OCR\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Apply threshold to get image with only black and white\n",
    "            _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            \n",
    "            # Noise removal\n",
    "            kernel = np.ones((1,1), np.uint8)\n",
    "            # opening = cv2.morphologyEx(thresh, cv2.MORPH_OPENING, kernel, iterations=1)\n",
    "            opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "            \n",
    "            # Extract text with specific config for tables\n",
    "            custom_config = r'--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz.,()$%-+ '\n",
    "            text = pytesseract.image_to_string(opening, config=custom_config)\n",
    "            \n",
    "            # Try to structure the extracted text as a table\n",
    "            lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "            \n",
    "            # Look for financial patterns (numbers with currency, percentages, etc.)\n",
    "            financial_lines = []\n",
    "            for line in lines:\n",
    "                if re.search(r'[\\d,]+\\.?\\d*[%$]?|^\\s*\\d', line):\n",
    "                    financial_lines.append(line)\n",
    "            \n",
    "            if financial_lines:\n",
    "                return '\\n'.join(financial_lines)\n",
    "            else:\n",
    "                return text if text.strip() else None\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting table from image {image_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_tables_advanced(self, doc, page_num: int, base_dir: str, items: List[Dict]):\n",
    "        \"\"\"\n",
    "        Enhanced table processing with multiple extraction methods for financial data.\n",
    "        \"\"\"\n",
    "        filepath = self.current_pdf_info['filepath']\n",
    "        \n",
    "        # Method 1: Tabula extraction (for text-based tables)\n",
    "        try:\n",
    "            tables = tabula.read_pdf(filepath, pages=page_num + 1, multiple_tables=True)\n",
    "            if tables:\n",
    "                for table_idx, table in enumerate(tables):\n",
    "                    # Enhanced table processing for financial data\n",
    "                    table_text = self._format_financial_table(table)\n",
    "                    table_file_name = f\"{base_dir}/tables/{os.path.basename(filepath)}_table_{page_num}_{table_idx}.txt\"\n",
    "                    \n",
    "                    with open(table_file_name, 'w', encoding='utf-8') as f:\n",
    "                        f.write(table_text)\n",
    "                    \n",
    "                    items.append({\n",
    "                        \"page\": page_num,\n",
    "                        \"type\": \"table\",\n",
    "                        \"text\": table_text,\n",
    "                        \"path\": table_file_name,\n",
    "                        \"extraction_method\": \"tabula\"\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Tabula extraction failed for page {page_num}: {e}\")\n",
    "        \n",
    "        # Method 2: Image-based table extraction\n",
    "        page = doc[page_num]\n",
    "        images = page.get_images()\n",
    "        \n",
    "        for idx, image in enumerate(images):\n",
    "            try:\n",
    "                xref = image[0]\n",
    "                pix = pymupdf.Pixmap(doc, xref)\n",
    "                image_path = f\"{base_dir}/images/{os.path.basename(filepath)}_image_{page_num}_{idx}_{xref}.png\"\n",
    "                pix.save(image_path)\n",
    "                \n",
    "                # Try to extract table data from this image\n",
    "                table_text = self.extract_table_from_image(image_path, page_num)\n",
    "                \n",
    "                if table_text and self._is_likely_financial_table(table_text):\n",
    "                    table_file_name = f\"{base_dir}/extracted_tables/{os.path.basename(filepath)}_img_table_{page_num}_{idx}.txt\"\n",
    "                    with open(table_file_name, 'w', encoding='utf-8') as f:\n",
    "                        f.write(table_text)\n",
    "                    \n",
    "                    items.append({\n",
    "                        \"page\": page_num,\n",
    "                        \"type\": \"table\",\n",
    "                        \"text\": table_text,\n",
    "                        \"path\": table_file_name,\n",
    "                        \"extraction_method\": \"ocr\",\n",
    "                        \"source_image\": image_path\n",
    "                    })\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing image {idx} on page {page_num}: {e}\")\n",
    "    \n",
    "    def _format_financial_table(self, table: pd.DataFrame) -> str:\n",
    "        \"\"\"Format pandas DataFrame as financial table with proper alignment\"\"\"\n",
    "        try:\n",
    "            # Clean and format the dataframe\n",
    "            table = table.fillna('')\n",
    "            \n",
    "            # Convert to string with proper formatting\n",
    "            formatted_rows = []\n",
    "            \n",
    "            # Add headers\n",
    "            headers = [str(col) for col in table.columns]\n",
    "            formatted_rows.append(\" | \".join(headers))\n",
    "            formatted_rows.append(\"-\" * len(\" | \".join(headers)))\n",
    "            \n",
    "            # Add data rows\n",
    "            for _, row in table.iterrows():\n",
    "                formatted_row = []\n",
    "                for value in row:\n",
    "                    if pd.isna(value):\n",
    "                        formatted_row.append(\"\")\n",
    "                    elif isinstance(value, (int, float)):\n",
    "                        # Format numbers properly\n",
    "                        if abs(value) >= 1000000:\n",
    "                            formatted_row.append(f\"{value:,.0f}\")\n",
    "                        elif abs(value) >= 1000:\n",
    "                            formatted_row.append(f\"{value:,.2f}\")\n",
    "                        else:\n",
    "                            formatted_row.append(f\"{value}\")\n",
    "                    else:\n",
    "                        formatted_row.append(str(value))\n",
    "                formatted_rows.append(\" | \".join(formatted_row))\n",
    "            \n",
    "            return \"\\n\".join(formatted_rows)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error formatting table: {e}\")\n",
    "            return str(table)\n",
    "    \n",
    "    def _is_likely_financial_table(self, text: str) -> bool:\n",
    "        \"\"\"Determine if extracted text likely contains financial table data\"\"\"\n",
    "        financial_keywords = [\n",
    "            'revenue', 'income', 'profit', 'loss', 'assets', 'liabilities',\n",
    "            'equity', 'cash', 'flow', 'balance', 'statement', 'fiscal',\n",
    "            'quarter', 'annual', 'million', 'billion', 'thousand', '$',\n",
    "            'expenses', 'costs', 'net', 'gross', 'total', 'year', 'ytd'\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check for financial keywords\n",
    "        keyword_score = sum(1 for keyword in financial_keywords if keyword in text_lower)\n",
    "        \n",
    "        # Check for numerical patterns typical in financial data\n",
    "        number_patterns = len(re.findall(r'\\$?[\\d,]+\\.?\\d*[MBK]?', text))\n",
    "        percentage_patterns = len(re.findall(r'\\d+\\.?\\d*%', text))\n",
    "        \n",
    "        return keyword_score >= 2 or number_patterns >= 3 or percentage_patterns >= 1\n",
    "    \n",
    "    def process_text_chunks(self, text: str, page_num: int, base_dir: str, items: List[Dict]):\n",
    "        \"\"\"Process text chunks with financial context awareness\"\"\"\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            text_file_name = f\"{base_dir}/text/{os.path.basename(self.current_pdf_info['filepath'])}_text_{page_num}_{i}.txt\"\n",
    "            \n",
    "            with open(text_file_name, 'w', encoding='utf-8') as f:\n",
    "                f.write(chunk)\n",
    "            \n",
    "            # Enhance chunk with financial context\n",
    "            chunk_type = \"financial_text\" if self._is_likely_financial_table(chunk) else \"text\"\n",
    "            \n",
    "            items.append({\n",
    "                \"page\": page_num,\n",
    "                \"type\": chunk_type,\n",
    "                \"text\": chunk,\n",
    "                \"path\": text_file_name\n",
    "            })\n",
    "    \n",
    "    def process_images(self, page, page_num: int, base_dir: str, items: List[Dict]):\n",
    "        \"\"\"Process images with base64 encoding\"\"\"\n",
    "        doc = page.parent\n",
    "        images = page.get_images()\n",
    "        \n",
    "        for idx, image in enumerate(images):\n",
    "            try:\n",
    "                xref = image[0]\n",
    "                pix = pymupdf.Pixmap(doc, xref)\n",
    "                image_name = f\"{base_dir}/images/{os.path.basename(self.current_pdf_info['filepath'])}_image_{page_num}_{idx}_{xref}.png\"\n",
    "                pix.save(image_name)\n",
    "                \n",
    "                with open(image_name, 'rb') as f:\n",
    "                    encoded_image = base64.b64encode(f.read()).decode('utf8')\n",
    "                \n",
    "                items.append({\n",
    "                    \"page\": page_num,\n",
    "                    \"type\": \"image\",\n",
    "                    \"path\": image_name,\n",
    "                    \"image\": encoded_image\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing image {idx} on page {page_num}: {e}\")\n",
    "    \n",
    "    def process_page_images(self, page, page_num: int, base_dir: str, items: List[Dict]):\n",
    "        \"\"\"Process full page images\"\"\"\n",
    "        try:\n",
    "            pix = page.get_pixmap()\n",
    "            page_path = os.path.join(base_dir, f\"page_images/page_{page_num:03d}.png\")\n",
    "            pix.save(page_path)\n",
    "            \n",
    "            with open(page_path, 'rb') as f:\n",
    "                page_image = base64.b64encode(f.read()).decode('utf8')\n",
    "            \n",
    "            items.append({\n",
    "                \"page\": page_num,\n",
    "                \"type\": \"page\",\n",
    "                \"path\": page_path,\n",
    "                \"image\": page_image\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing page image {page_num}: {e}\")\n",
    "    \n",
    "    # def generate_multimodal_embeddings(self, prompt: str = None, image: str = None) -> Optional[List[float]]:\n",
    "    #     \"\"\"Generate embeddings using Amazon Titan\"\"\"\n",
    "    #     if not prompt and not image:\n",
    "    #         raise ValueError(\"Please provide either text prompt, base64 image, or both\")\n",
    "        \n",
    "    #     body = {\"embeddingConfig\": {\"outputEmbeddingLength\": self.embedding_dimension}}\n",
    "        \n",
    "    #     if prompt:\n",
    "    #         body[\"inputText\"] = prompt\n",
    "    #     if image:\n",
    "    #         body[\"inputImage\"] = image\n",
    "        \n",
    "    #     try:\n",
    "    #         response = self.bedrock_client.invoke_model(\n",
    "    #             modelId=self.model_id,\n",
    "    #             body=json.dumps(body),\n",
    "    #             accept=\"application/json\",\n",
    "    #             contentType=\"application/json\"\n",
    "    #         )\n",
    "            \n",
    "    #         result = json.loads(response.get(\"body\").read())\n",
    "    #         return result.get(\"embedding\")\n",
    "            \n",
    "    #     except ClientError as err:\n",
    "    #         logger.error(f\"Couldn't invoke Titan embedding model: {err.response['Error']['Message']}\")\n",
    "    #         return None\n",
    "    \n",
    "    def generate_multimodal_embeddings(self, prompt: str = None, image: str = None) -> Optional[List[float]]:\n",
    "        if prompt:\n",
    "            return self.embedding_model.encode(prompt).tolist()\n",
    "        return None  # Skip image embeddings for now\n",
    "    \n",
    "    def process_pdf(self, filepath: str) -> Dict:\n",
    "        \"\"\"Main PDF processing function\"\"\"\n",
    "        self.current_pdf_info = {\n",
    "            'filepath': filepath,\n",
    "            'filename': os.path.basename(filepath)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n=== Processing PDF: {self.current_pdf_info['filename']} ===\")\n",
    "        \n",
    "        doc = pymupdf.open(filepath)\n",
    "        num_pages = len(doc)\n",
    "        base_dir = f\"data_{Path(filepath).stem}\"\n",
    "        \n",
    "        self.create_directories(base_dir)\n",
    "        self.items = []\n",
    "        \n",
    "        # Process each page\n",
    "        for page_num in tqdm(range(num_pages), desc=\"Processing PDF pages\"):\n",
    "            page = doc[page_num]\n",
    "            text = page.get_text()\n",
    "            \n",
    "            # Enhanced table processing\n",
    "            self.process_tables_advanced(doc, page_num, base_dir, self.items)\n",
    "            \n",
    "            # Process text chunks\n",
    "            self.process_text_chunks(text, page_num, base_dir, self.items)\n",
    "            \n",
    "            # Process images\n",
    "            self.process_images(page, page_num, base_dir, self.items)\n",
    "            \n",
    "            # Process page images\n",
    "            self.process_page_images(page, page_num, base_dir, self.items)\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        # Generate embeddings\n",
    "        self._generate_all_embeddings()\n",
    "        \n",
    "        # Create FAISS index\n",
    "        self._create_faiss_index()\n",
    "        \n",
    "        print(f\"✓ Processed {len(self.items)} items from {num_pages} pages\")\n",
    "        self._print_processing_summary()\n",
    "        \n",
    "        return {\n",
    "            'filepath': filepath,\n",
    "            'items_count': len(self.items),\n",
    "            'pages': num_pages,\n",
    "            'base_dir': base_dir\n",
    "        }\n",
    "    \n",
    "    def _generate_all_embeddings(self):\n",
    "        \"\"\"Generate embeddings for all processed items\"\"\"\n",
    "        print(\"\\n=== Generating Embeddings ===\")\n",
    "        \n",
    "        item_counts = {\n",
    "            'text': sum(1 for item in self.items if item['type'] in ['text', 'financial_text']),\n",
    "            'table': sum(1 for item in self.items if item['type'] == 'table'),\n",
    "            'image': sum(1 for item in self.items if item['type'] == 'image'),\n",
    "            'page': sum(1 for item in self.items if item['type'] == 'page')\n",
    "        }\n",
    "        \n",
    "        counters = dict.fromkeys(item_counts.keys(), 0)\n",
    "        \n",
    "        with tqdm(total=len(self.items), desc=\"Generating embeddings\") as pbar:\n",
    "            for item in self.items:\n",
    "                item_type = item['type']\n",
    "                \n",
    "                if item_type in ['text', 'table', 'financial_text']:\n",
    "                    embedding = self.generate_multimodal_embeddings(prompt=item['text'])\n",
    "                    counters['text' if item_type != 'table' else 'table'] += 1\n",
    "                else:\n",
    "                    embedding = self.generate_multimodal_embeddings(image=item['image'])\n",
    "                    counters[item_type] += 1\n",
    "                \n",
    "                item['embedding'] = embedding\n",
    "                \n",
    "                pbar.set_postfix_str(\n",
    "                    f\"Text: {counters['text']}/{item_counts['text']}, \"\n",
    "                    f\"Table: {counters['table']}/{item_counts['table']}, \"\n",
    "                    f\"Image: {counters['image']}/{item_counts['image']}\"\n",
    "                )\n",
    "                pbar.update(1)\n",
    "    \n",
    "    def _create_faiss_index(self):\n",
    "        \"\"\"Create and populate FAISS index\"\"\"\n",
    "        all_embeddings = np.array([item['embedding'] for item in self.items if item['embedding']])\n",
    "        \n",
    "        self.index = faiss.IndexFlatL2(self.embedding_dimension)\n",
    "        self.index.add(np.array(all_embeddings, dtype=np.float32))\n",
    "        \n",
    "        print(f\"✓ Created FAISS index with {self.index.ntotal} embeddings\")\n",
    "    \n",
    "    def _print_processing_summary(self):\n",
    "        \"\"\"Print summary of processed items\"\"\"\n",
    "        summary = {}\n",
    "        for item in self.items:\n",
    "            item_type = item['type']\n",
    "            summary[item_type] = summary.get(item_type, 0) + 1\n",
    "        \n",
    "        print(\"\\n=== Processing Summary ===\")\n",
    "        for item_type, count in summary.items():\n",
    "            print(f\"{item_type.capitalize()}: {count}\")\n",
    "    \n",
    "    def query_documents(self, query: str, k: int = 15) -> str:\n",
    "        \"\"\"Query the processed documents using RAG\"\"\"\n",
    "        if not self.index or not self.items:\n",
    "            return \"No documents processed yet. Please process a PDF first.\"\n",
    "        \n",
    "        print(f\"\\n=== Querying: {query} ===\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.generate_multimodal_embeddings(prompt=query)\n",
    "        if not query_embedding:\n",
    "            return \"Failed to generate query embedding.\"\n",
    "        \n",
    "        # Search for similar items\n",
    "        distances, result = self.index.search(\n",
    "            np.array(query_embedding, dtype=np.float32).reshape(1, -1), k=k\n",
    "        )\n",
    "        \n",
    "        # Get matched items\n",
    "        matched_items = []\n",
    "        for idx in result.flatten():\n",
    "            if idx < len(self.items):\n",
    "                item = {k: v for k, v in self.items[idx].items() if k != 'embedding'}\n",
    "                matched_items.append(item)\n",
    "        \n",
    "        # Generate response using Nova\n",
    "        response = self._invoke_nova_multimodal(query, matched_items)\n",
    "        \n",
    "        print(f\"✓ Found {len(matched_items)} relevant items\")\n",
    "        return response\n",
    "    \n",
    "    # def _invoke_nova_multimodal(self, prompt: str, matched_items: List[Dict]) -> str:\n",
    "    #     context = \"\\n\".join([f\"[Page {item['page']}] {item['text']}\" for item in matched_items if item['type'] in ['text', 'table']])\n",
    "        \n",
    "    #     try:\n",
    "    #         result = self.qa_pipeline(question=prompt, context=context[:2000])  # Limit context length\n",
    "    #         return f\"{result['answer']} (confidence: {result['score']:.2f})\"\n",
    "    #     except:\n",
    "    #         return f\"Based on the documents:\\n{context[:500]}...\"\n",
    "        \n",
    "    def _invoke_nova_multimodal(self, prompt: str, matched_items: List[Dict]) -> str:\n",
    "        # Build and clean context\n",
    "        context_parts = []\n",
    "        \n",
    "        for item in matched_items:\n",
    "            if item['type'] in ['text', 'table', 'financial_text']:\n",
    "                # Clean table formatting\n",
    "                text = item['text']\n",
    "                text = re.sub(r'\\s*\\|\\s*', ' ', text)  # Remove pipe separators\n",
    "                text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "                context_parts.append(f\"Page {item['page']}: {text}\")\n",
    "        \n",
    "        full_context = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Extract specific relevant information\n",
    "        prompt_lower = prompt.lower()\n",
    "        relevant_info = []\n",
    "        confidence_score = 0.0\n",
    "        \n",
    "        # Search for direct answers\n",
    "        for line in full_context.split('\\n'):\n",
    "            line_lower = line.lower()\n",
    "            \n",
    "            # Check for year patterns in query\n",
    "            year_match = re.search(r'20\\d{2}', prompt)\n",
    "            if year_match:\n",
    "                year = year_match.group()\n",
    "                if year in line and any(keyword in line_lower for keyword in ['revenue', 'ebitda', 'profit', 'sales', 'income']):\n",
    "                    relevant_info.append(line.strip())\n",
    "                    confidence_score += 0.3\n",
    "            \n",
    "            # Check for financial metrics\n",
    "            if any(metric in prompt_lower for metric in ['revenue', 'ebitda', 'profit', 'income']):\n",
    "                if re.search(r'\\d+\\.?\\d*', line) and any(metric in line_lower for metric in ['revenue', 'ebitda', 'profit', 'income', 'sales']):\n",
    "                    relevant_info.append(line.strip())\n",
    "                    confidence_score += 0.2\n",
    "        \n",
    "        # Prepare context for generation\n",
    "        if relevant_info:\n",
    "            context_for_generation = \"\\n\".join(relevant_info[:10])\n",
    "            confidence_score = min(confidence_score, 1.0)\n",
    "        else:\n",
    "            context_for_generation = full_context[:1500]\n",
    "            confidence_score = 0.1\n",
    "        \n",
    "        # Generate answer with better prompt\n",
    "        if \"how many years\" in prompt_lower:\n",
    "            # Extract all years mentioned\n",
    "            years = sorted(set(re.findall(r'20\\d{2}', full_context)))\n",
    "            if years:\n",
    "                answer = f\"The document contains data for {len(years)} years: {', '.join(years)}\"\n",
    "                confidence_score = 0.9\n",
    "            else:\n",
    "                answer = \"Could not determine the number of years covered in the document.\"\n",
    "                confidence_score = 0.2\n",
    "        \n",
    "        elif \"what is this pdf about\" in prompt_lower:\n",
    "            # Look for title/header information\n",
    "            input_text = f\"Summarize what this document is about based on the following content:\\n\\n{context_for_generation[:1000]}\\n\\nSummary:\"\n",
    "            try:\n",
    "                response = self.generator(input_text, max_new_tokens=100, min_length=20, do_sample=False)\n",
    "                answer = response[0]['generated_text']\n",
    "                confidence_score = 0.7\n",
    "            except:\n",
    "                answer = \"This appears to be a financial/sustainability report containing performance metrics and data.\"\n",
    "                confidence_score = 0.5\n",
    "        \n",
    "        else:\n",
    "            # For specific queries\n",
    "            input_text = f\"\"\"Based on the following data, answer the question precisely. If there are numbers, include them with units.\n",
    "\n",
    "    Data:\n",
    "    {context_for_generation}\n",
    "\n",
    "    Question: {prompt}\n",
    "\n",
    "    Direct answer:\"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = self.generator(input_text, max_new_tokens=50, min_length=5, do_sample=False)\n",
    "                generated = response[0]['generated_text'].strip()\n",
    "                \n",
    "                # Extract the most relevant number if found\n",
    "                numbers_in_context = re.findall(r'\\d+\\.?\\d*', context_for_generation)\n",
    "                if numbers_in_context and generated:\n",
    "                    answer = generated\n",
    "                    confidence_score = max(confidence_score, 0.6)\n",
    "                else:\n",
    "                    answer = generated if generated else \"No specific data found.\"\n",
    "                    confidence_score = max(confidence_score, 0.3)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # Fallback: extract the most relevant line\n",
    "                if relevant_info:\n",
    "                    answer = relevant_info[0]\n",
    "                    confidence_score = 0.4\n",
    "                else:\n",
    "                    answer = \"Could not extract specific information for this query.\"\n",
    "                    confidence_score = 0.1\n",
    "        \n",
    "        # Format final response with confidence\n",
    "        confidence_text = \"High\" if confidence_score > 0.7 else \"Medium\" if confidence_score > 0.4 else \"Low\"\n",
    "        \n",
    "        return f\"\"\"Answer: {answer}\n",
    "\n",
    "    Confidence: {confidence_text} ({confidence_score:.1%})\n",
    "    Source pages: {', '.join(str(item['page']) for item in matched_items[:3])}\n",
    "\n",
    "    {f\"Additional context: {relevant_info[1]}\" if len(relevant_info) > 1 else \"\"}\"\"\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the financial PDF processor\"\"\"\n",
    "    processor = FinancialPDFProcessor()\n",
    "    \n",
    "    print(\"=== Financial PDF Data Extractor with RAG ===\")\n",
    "    print(\"Specialized for precise financial data extraction from PDFs\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n=== Main Menu ===\")\n",
    "        print(\"1. Process new PDF\")\n",
    "        print(\"2. Query current PDF\")\n",
    "        print(\"3. Exit\")\n",
    "        \n",
    "        choice = input(\"Choose option (1-3): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            try:\n",
    "                filepath, source_type = processor.select_pdf_source()\n",
    "                result = processor.process_pdf(filepath)\n",
    "                print(f\"\\n✓ Successfully processed: {result['filepath']}\")\n",
    "                print(f\"  - {result['items_count']} items extracted\")\n",
    "                print(f\"  - {result['pages']} pages processed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error processing PDF: {e}\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            if not processor.items:\n",
    "                print(\"No PDF processed yet. Please process a PDF first.\")\n",
    "                continue\n",
    "            \n",
    "            query = input(\"\\nEnter your question: \").strip()\n",
    "            if query:\n",
    "                response = processor.query_documents(query)\n",
    "                print(f\"\\n=== Response ===\\n{response}\")\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7cb499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
