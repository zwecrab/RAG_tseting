{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b2047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tabula\n",
    "import faiss\n",
    "import json\n",
    "import base64\n",
    "import pymupdf\n",
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import pytesseract\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_aws import ChatBedrock\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import re\n",
    "from pathlib import Path\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66757a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-24'  # Replace XX with your version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d3100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class FinancialPDFProcessor:\n",
    "    \"\"\"\n",
    "    A comprehensive PDF processor specialized for financial documents with multi-modal RAG capabilities.\n",
    "    Handles both text-based and image-based tables with high precision.\n",
    "    \"\"\"\n",
    "    \n",
    "    # def __init__(self, embedding_dimension: int = 384, chunk_size: int = 700, chunk_overlap: int = 200):\n",
    "    #     self.embedding_dimension = embedding_dimension\n",
    "    #     self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #         chunk_size=chunk_size, \n",
    "    #         chunk_overlap=chunk_overlap, \n",
    "    #         length_function=len\n",
    "    #     )\n",
    "    #     self.bedrock_client = boto3.client(service_name=\"bedrock-runtime\", region_name='us-east-1')\n",
    "    #     self.model_id = \"amazon.titan-embed-image-v1\"\n",
    "    #     self.llm_model_id = \"amazon.nova-pro-v1:0\"\n",
    "    #     self.index = None\n",
    "    #     self.items = []\n",
    "    #     self.current_pdf_info = {}\n",
    "    \n",
    "    def __init__(self, embedding_dimension: int = 384, chunk_size: int = 700, chunk_overlap: int = 200):\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap, \n",
    "            length_function=len\n",
    "        )\n",
    "        # Replace AWS with free models\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "        self.qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "        self.index = None\n",
    "        self.items = []\n",
    "        self.current_pdf_info = {}\n",
    "        \n",
    "    def select_pdf_source(self) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Interactive PDF selection - either from URL or local file.\n",
    "        Returns: (filepath, source_type)\n",
    "        \"\"\"\n",
    "        print(\"\\n=== PDF Source Selection ===\")\n",
    "        print(\"1. Download from URL\")\n",
    "        print(\"2. Use local file\")\n",
    "        print(\"3. Select from common financial document URLs\")\n",
    "        \n",
    "        choice = input(\"Choose option (1-3): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            return self._download_from_url()\n",
    "        elif choice == \"2\":\n",
    "            return self._select_local_file()\n",
    "        elif choice == \"3\":\n",
    "            return self._select_from_common_urls()\n",
    "        else:\n",
    "            print(\"Invalid choice. Using local file selection.\")\n",
    "            return self._select_local_file()\n",
    "    \n",
    "    def _download_from_url(self) -> Tuple[str, str]:\n",
    "        \"\"\"Download PDF from URL\"\"\"\n",
    "        url = input(\"Enter PDF URL: \").strip()\n",
    "        filename = input(\"Enter filename (with .pdf extension): \").strip()\n",
    "        \n",
    "        if not filename.endswith('.pdf'):\n",
    "            filename += '.pdf'\n",
    "            \n",
    "        filepath = os.path.join(\"data\", filename)\n",
    "        os.makedirs(\"data\", exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(filepath, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            \n",
    "            print(f\"✓ Downloaded: {filepath}\")\n",
    "            return filepath, \"url\"\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Download failed: {e}\")\n",
    "            return self._select_local_file()\n",
    "    \n",
    "    def _select_local_file(self) -> Tuple[str, str]:\n",
    "        \"\"\"Select local PDF file\"\"\"\n",
    "        filepath = input(\"Enter path to local PDF file: \").strip()\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"✗ File not found: {filepath}\")\n",
    "            # List available PDFs in current directory\n",
    "            pdf_files = [f for f in os.listdir('.') if f.endswith('.pdf')]\n",
    "            if pdf_files:\n",
    "                print(\"\\nAvailable PDF files in current directory:\")\n",
    "                for i, pdf in enumerate(pdf_files, 1):\n",
    "                    print(f\"{i}. {pdf}\")\n",
    "                try:\n",
    "                    choice = int(input(\"Select file number: \")) - 1\n",
    "                    filepath = pdf_files[choice]\n",
    "                except (ValueError, IndexError):\n",
    "                    raise FileNotFoundError(\"No valid PDF file selected\")\n",
    "            else:\n",
    "                raise FileNotFoundError(\"No PDF files found\")\n",
    "        \n",
    "        print(f\"✓ Selected: {filepath}\")\n",
    "        return filepath, \"local\"\n",
    "    \n",
    "    def _select_from_common_urls(self) -> Tuple[str, str]:\n",
    "        \"\"\"Select from predefined financial document URLs\"\"\"\n",
    "        common_urls = {\n",
    "            \"1\": (\"Sample Annual Report\", \"https://www.sec.gov/Archives/edgar/data/320193/000032019323000077/aapl-20230930.htm\"),\n",
    "            \"2\": (\"Financial Statement Sample\", \"https://arxiv.org/pdf/1706.03762.pdf\"),  # Placeholder\n",
    "            \"3\": (\"Custom URL\", \"\")\n",
    "        }\n",
    "        \n",
    "        print(\"\\nCommon Financial Documents:\")\n",
    "        for key, (name, url) in common_urls.items():\n",
    "            print(f\"{key}. {name}\")\n",
    "        \n",
    "        choice = input(\"Select option: \").strip()\n",
    "        \n",
    "        if choice == \"3\":\n",
    "            return self._download_from_url()\n",
    "        elif choice in common_urls:\n",
    "            name, url = common_urls[choice]\n",
    "            filename = f\"{name.lower().replace(' ', '_')}.pdf\"\n",
    "            filepath = os.path.join(\"data\", filename)\n",
    "            \n",
    "            # Download logic here\n",
    "            return self._download_specific_url(url, filepath)\n",
    "        else:\n",
    "            return self._download_from_url()\n",
    "    \n",
    "    def _download_specific_url(self, url: str, filepath: str) -> Tuple[str, str]:\n",
    "        \"\"\"Download from specific URL\"\"\"\n",
    "        os.makedirs(\"data\", exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(filepath, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            \n",
    "            print(f\"✓ Downloaded: {filepath}\")\n",
    "            return filepath, \"url\"\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Download failed: {e}\")\n",
    "            return self._select_local_file()\n",
    "    \n",
    "    def create_directories(self, base_dir: str):\n",
    "        \"\"\"Create necessary directories for processing\"\"\"\n",
    "        directories = [\"images\", \"text\", \"tables\", \"page_images\", \"extracted_tables\"]\n",
    "        for directory in directories:\n",
    "            os.makedirs(os.path.join(base_dir, directory), exist_ok=True)\n",
    "    \n",
    "    def extract_table_from_image(self, image_path: str, page_num: int) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract tabular data from image using OCR with enhanced preprocessing for financial data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                return None\n",
    "            \n",
    "            # Preprocessing for better OCR\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Apply threshold to get image with only black and white\n",
    "            _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            \n",
    "            # Noise removal\n",
    "            kernel = np.ones((1,1), np.uint8)\n",
    "            # opening = cv2.morphologyEx(thresh, cv2.MORPH_OPENING, kernel, iterations=1)\n",
    "            opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "            \n",
    "            # Extract text with specific config for tables\n",
    "            custom_config = r'--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz.,()$%-+ '\n",
    "            text = pytesseract.image_to_string(opening, config=custom_config)\n",
    "            \n",
    "            # Try to structure the extracted text as a table\n",
    "            lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "            \n",
    "            # Look for financial patterns (numbers with currency, percentages, etc.)\n",
    "            financial_lines = []\n",
    "            for line in lines:\n",
    "                if re.search(r'[\\d,]+\\.?\\d*[%$]?|^\\s*\\d', line):\n",
    "                    financial_lines.append(line)\n",
    "            \n",
    "            if financial_lines:\n",
    "                return '\\n'.join(financial_lines)\n",
    "            else:\n",
    "                return text if text.strip() else None\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting table from image {image_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_tables_advanced(self, doc, page_num: int, base_dir: str, items: List[Dict]):\n",
    "        \"\"\"\n",
    "        Enhanced table processing with multiple extraction methods for financial data.\n",
    "        \"\"\"\n",
    "        filepath = self.current_pdf_info['filepath']\n",
    "        \n",
    "        # Method 1: Tabula extraction (for text-based tables)\n",
    "        try:\n",
    "            tables = tabula.read_pdf(filepath, pages=page_num + 1, multiple_tables=True)\n",
    "            if tables:\n",
    "                for table_idx, table in enumerate(tables):\n",
    "                    # Enhanced table processing for financial data\n",
    "                    table_text = self._format_financial_table(table)\n",
    "                    table_file_name = f\"{base_dir}/tables/{os.path.basename(filepath)}_table_{page_num}_{table_idx}.txt\"\n",
    "                    \n",
    "                    with open(table_file_name, 'w', encoding='utf-8') as f:\n",
    "                        f.write(table_text)\n",
    "                    \n",
    "                    items.append({\n",
    "                        \"page\": page_num,\n",
    "                        \"type\": \"table\",\n",
    "                        \"text\": table_text,\n",
    "                        \"path\": table_file_name,\n",
    "                        \"extraction_method\": \"tabula\"\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Tabula extraction failed for page {page_num}: {e}\")\n",
    "        \n",
    "        # Method 2: Image-based table extraction\n",
    "        page = doc[page_num]\n",
    "        images = page.get_images()\n",
    "        \n",
    "        for idx, image in enumerate(images):\n",
    "            try:\n",
    "                xref = image[0]\n",
    "                pix = pymupdf.Pixmap(doc, xref)\n",
    "                image_path = f\"{base_dir}/images/{os.path.basename(filepath)}_image_{page_num}_{idx}_{xref}.png\"\n",
    "                pix.save(image_path)\n",
    "                \n",
    "                # Try to extract table data from this image\n",
    "                table_text = self.extract_table_from_image(image_path, page_num)\n",
    "                \n",
    "                if table_text and self._is_likely_financial_table(table_text):\n",
    "                    table_file_name = f\"{base_dir}/extracted_tables/{os.path.basename(filepath)}_img_table_{page_num}_{idx}.txt\"\n",
    "                    with open(table_file_name, 'w', encoding='utf-8') as f:\n",
    "                        f.write(table_text)\n",
    "                    \n",
    "                    items.append({\n",
    "                        \"page\": page_num,\n",
    "                        \"type\": \"table\",\n",
    "                        \"text\": table_text,\n",
    "                        \"path\": table_file_name,\n",
    "                        \"extraction_method\": \"ocr\",\n",
    "                        \"source_image\": image_path\n",
    "                    })\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing image {idx} on page {page_num}: {e}\")\n",
    "    \n",
    "    def _format_financial_table(self, table: pd.DataFrame) -> str:\n",
    "        \"\"\"Format pandas DataFrame as financial table with proper alignment\"\"\"\n",
    "        try:\n",
    "            # Clean and format the dataframe\n",
    "            table = table.fillna('')\n",
    "            \n",
    "            # Convert to string with proper formatting\n",
    "            formatted_rows = []\n",
    "            \n",
    "            # Add headers\n",
    "            headers = [str(col) for col in table.columns]\n",
    "            formatted_rows.append(\" | \".join(headers))\n",
    "            formatted_rows.append(\"-\" * len(\" | \".join(headers)))\n",
    "            \n",
    "            # Add data rows\n",
    "            for _, row in table.iterrows():\n",
    "                formatted_row = []\n",
    "                for value in row:\n",
    "                    if pd.isna(value):\n",
    "                        formatted_row.append(\"\")\n",
    "                    elif isinstance(value, (int, float)):\n",
    "                        # Format numbers properly\n",
    "                        if abs(value) >= 1000000:\n",
    "                            formatted_row.append(f\"{value:,.0f}\")\n",
    "                        elif abs(value) >= 1000:\n",
    "                            formatted_row.append(f\"{value:,.2f}\")\n",
    "                        else:\n",
    "                            formatted_row.append(f\"{value}\")\n",
    "                    else:\n",
    "                        formatted_row.append(str(value))\n",
    "                formatted_rows.append(\" | \".join(formatted_row))\n",
    "            \n",
    "            return \"\\n\".join(formatted_rows)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error formatting table: {e}\")\n",
    "            return str(table)\n",
    "    \n",
    "    def _is_likely_financial_table(self, text: str) -> bool:\n",
    "        \"\"\"Determine if extracted text likely contains financial table data\"\"\"\n",
    "        financial_keywords = [\n",
    "            'revenue', 'income', 'profit', 'loss', 'assets', 'liabilities',\n",
    "            'equity', 'cash', 'flow', 'balance', 'statement', 'fiscal',\n",
    "            'quarter', 'annual', 'million', 'billion', 'thousand', '$',\n",
    "            'expenses', 'costs', 'net', 'gross', 'total', 'year', 'ytd'\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check for financial keywords\n",
    "        keyword_score = sum(1 for keyword in financial_keywords if keyword in text_lower)\n",
    "        \n",
    "        # Check for numerical patterns typical in financial data\n",
    "        number_patterns = len(re.findall(r'\\$?[\\d,]+\\.?\\d*[MBK]?', text))\n",
    "        percentage_patterns = len(re.findall(r'\\d+\\.?\\d*%', text))\n",
    "        \n",
    "        return keyword_score >= 2 or number_patterns >= 3 or percentage_patterns >= 1\n",
    "    \n",
    "    def process_text_chunks(self, text: str, page_num: int, base_dir: str, items: List[Dict]):\n",
    "        \"\"\"Process text chunks with financial context awareness\"\"\"\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            text_file_name = f\"{base_dir}/text/{os.path.basename(self.current_pdf_info['filepath'])}_text_{page_num}_{i}.txt\"\n",
    "            \n",
    "            with open(text_file_name, 'w', encoding='utf-8') as f:\n",
    "                f.write(chunk)\n",
    "            \n",
    "            # Enhance chunk with financial context\n",
    "            chunk_type = \"financial_text\" if self._is_likely_financial_table(chunk) else \"text\"\n",
    "            \n",
    "            items.append({\n",
    "                \"page\": page_num,\n",
    "                \"type\": chunk_type,\n",
    "                \"text\": chunk,\n",
    "                \"path\": text_file_name\n",
    "            })\n",
    "    \n",
    "    def process_images(self, page, page_num: int, base_dir: str, items: List[Dict]):\n",
    "        \"\"\"Process images with base64 encoding\"\"\"\n",
    "        doc = page.parent\n",
    "        images = page.get_images()\n",
    "        \n",
    "        for idx, image in enumerate(images):\n",
    "            try:\n",
    "                xref = image[0]\n",
    "                pix = pymupdf.Pixmap(doc, xref)\n",
    "                image_name = f\"{base_dir}/images/{os.path.basename(self.current_pdf_info['filepath'])}_image_{page_num}_{idx}_{xref}.png\"\n",
    "                pix.save(image_name)\n",
    "                \n",
    "                with open(image_name, 'rb') as f:\n",
    "                    encoded_image = base64.b64encode(f.read()).decode('utf8')\n",
    "                \n",
    "                items.append({\n",
    "                    \"page\": page_num,\n",
    "                    \"type\": \"image\",\n",
    "                    \"path\": image_name,\n",
    "                    \"image\": encoded_image\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing image {idx} on page {page_num}: {e}\")\n",
    "    \n",
    "    def process_page_images(self, page, page_num: int, base_dir: str, items: List[Dict]):\n",
    "        \"\"\"Process full page images\"\"\"\n",
    "        try:\n",
    "            pix = page.get_pixmap()\n",
    "            page_path = os.path.join(base_dir, f\"page_images/page_{page_num:03d}.png\")\n",
    "            pix.save(page_path)\n",
    "            \n",
    "            with open(page_path, 'rb') as f:\n",
    "                page_image = base64.b64encode(f.read()).decode('utf8')\n",
    "            \n",
    "            items.append({\n",
    "                \"page\": page_num,\n",
    "                \"type\": \"page\",\n",
    "                \"path\": page_path,\n",
    "                \"image\": page_image\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing page image {page_num}: {e}\")\n",
    "    \n",
    "    # def generate_multimodal_embeddings(self, prompt: str = None, image: str = None) -> Optional[List[float]]:\n",
    "    #     \"\"\"Generate embeddings using Amazon Titan\"\"\"\n",
    "    #     if not prompt and not image:\n",
    "    #         raise ValueError(\"Please provide either text prompt, base64 image, or both\")\n",
    "        \n",
    "    #     body = {\"embeddingConfig\": {\"outputEmbeddingLength\": self.embedding_dimension}}\n",
    "        \n",
    "    #     if prompt:\n",
    "    #         body[\"inputText\"] = prompt\n",
    "    #     if image:\n",
    "    #         body[\"inputImage\"] = image\n",
    "        \n",
    "    #     try:\n",
    "    #         response = self.bedrock_client.invoke_model(\n",
    "    #             modelId=self.model_id,\n",
    "    #             body=json.dumps(body),\n",
    "    #             accept=\"application/json\",\n",
    "    #             contentType=\"application/json\"\n",
    "    #         )\n",
    "            \n",
    "    #         result = json.loads(response.get(\"body\").read())\n",
    "    #         return result.get(\"embedding\")\n",
    "            \n",
    "    #     except ClientError as err:\n",
    "    #         logger.error(f\"Couldn't invoke Titan embedding model: {err.response['Error']['Message']}\")\n",
    "    #         return None\n",
    "    \n",
    "    def generate_multimodal_embeddings(self, prompt: str = None, image: str = None) -> Optional[List[float]]:\n",
    "        if prompt:\n",
    "            return self.embedding_model.encode(prompt).tolist()\n",
    "        return None  # Skip image embeddings for now\n",
    "    \n",
    "    def process_pdf(self, filepath: str) -> Dict:\n",
    "        \"\"\"Main PDF processing function\"\"\"\n",
    "        self.current_pdf_info = {\n",
    "            'filepath': filepath,\n",
    "            'filename': os.path.basename(filepath)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n=== Processing PDF: {self.current_pdf_info['filename']} ===\")\n",
    "        \n",
    "        doc = pymupdf.open(filepath)\n",
    "        num_pages = len(doc)\n",
    "        base_dir = f\"data_{Path(filepath).stem}\"\n",
    "        \n",
    "        self.create_directories(base_dir)\n",
    "        self.items = []\n",
    "        \n",
    "        # Process each page\n",
    "        for page_num in tqdm(range(num_pages), desc=\"Processing PDF pages\"):\n",
    "            page = doc[page_num]\n",
    "            text = page.get_text()\n",
    "            \n",
    "            # Enhanced table processing\n",
    "            self.process_tables_advanced(doc, page_num, base_dir, self.items)\n",
    "            \n",
    "            # Process text chunks\n",
    "            self.process_text_chunks(text, page_num, base_dir, self.items)\n",
    "            \n",
    "            # Process images\n",
    "            self.process_images(page, page_num, base_dir, self.items)\n",
    "            \n",
    "            # Process page images\n",
    "            self.process_page_images(page, page_num, base_dir, self.items)\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        # Generate embeddings\n",
    "        self._generate_all_embeddings()\n",
    "        \n",
    "        # Create FAISS index\n",
    "        self._create_faiss_index()\n",
    "        \n",
    "        print(f\"✓ Processed {len(self.items)} items from {num_pages} pages\")\n",
    "        self._print_processing_summary()\n",
    "        \n",
    "        return {\n",
    "            'filepath': filepath,\n",
    "            'items_count': len(self.items),\n",
    "            'pages': num_pages,\n",
    "            'base_dir': base_dir\n",
    "        }\n",
    "    \n",
    "    def _generate_all_embeddings(self):\n",
    "        \"\"\"Generate embeddings for all processed items\"\"\"\n",
    "        print(\"\\n=== Generating Embeddings ===\")\n",
    "        \n",
    "        item_counts = {\n",
    "            'text': sum(1 for item in self.items if item['type'] in ['text', 'financial_text']),\n",
    "            'table': sum(1 for item in self.items if item['type'] == 'table'),\n",
    "            'image': sum(1 for item in self.items if item['type'] == 'image'),\n",
    "            'page': sum(1 for item in self.items if item['type'] == 'page')\n",
    "        }\n",
    "        \n",
    "        counters = dict.fromkeys(item_counts.keys(), 0)\n",
    "        \n",
    "        with tqdm(total=len(self.items), desc=\"Generating embeddings\") as pbar:\n",
    "            for item in self.items:\n",
    "                item_type = item['type']\n",
    "                \n",
    "                if item_type in ['text', 'table', 'financial_text']:\n",
    "                    embedding = self.generate_multimodal_embeddings(prompt=item['text'])\n",
    "                    counters['text' if item_type != 'table' else 'table'] += 1\n",
    "                else:\n",
    "                    embedding = self.generate_multimodal_embeddings(image=item['image'])\n",
    "                    counters[item_type] += 1\n",
    "                \n",
    "                item['embedding'] = embedding\n",
    "                \n",
    "                pbar.set_postfix_str(\n",
    "                    f\"Text: {counters['text']}/{item_counts['text']}, \"\n",
    "                    f\"Table: {counters['table']}/{item_counts['table']}, \"\n",
    "                    f\"Image: {counters['image']}/{item_counts['image']}\"\n",
    "                )\n",
    "                pbar.update(1)\n",
    "    \n",
    "    def _create_faiss_index(self):\n",
    "        \"\"\"Create and populate FAISS index\"\"\"\n",
    "        all_embeddings = np.array([item['embedding'] for item in self.items if item['embedding']])\n",
    "        \n",
    "        self.index = faiss.IndexFlatL2(self.embedding_dimension)\n",
    "        self.index.add(np.array(all_embeddings, dtype=np.float32))\n",
    "        \n",
    "        print(f\"✓ Created FAISS index with {self.index.ntotal} embeddings\")\n",
    "    \n",
    "    def _print_processing_summary(self):\n",
    "        \"\"\"Print summary of processed items\"\"\"\n",
    "        summary = {}\n",
    "        for item in self.items:\n",
    "            item_type = item['type']\n",
    "            summary[item_type] = summary.get(item_type, 0) + 1\n",
    "        \n",
    "        print(\"\\n=== Processing Summary ===\")\n",
    "        for item_type, count in summary.items():\n",
    "            print(f\"{item_type.capitalize()}: {count}\")\n",
    "    \n",
    "    def query_documents(self, query: str, k: int = 10) -> str:\n",
    "        \"\"\"Query the processed documents using RAG\"\"\"\n",
    "        if not self.index or not self.items:\n",
    "            return \"No documents processed yet. Please process a PDF first.\"\n",
    "        \n",
    "        print(f\"\\n=== Querying: {query} ===\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.generate_multimodal_embeddings(prompt=query)\n",
    "        if not query_embedding:\n",
    "            return \"Failed to generate query embedding.\"\n",
    "        \n",
    "        # Search for similar items\n",
    "        distances, result = self.index.search(\n",
    "            np.array(query_embedding, dtype=np.float32).reshape(1, -1), k=k\n",
    "        )\n",
    "        \n",
    "        # Get matched items\n",
    "        matched_items = []\n",
    "        for idx in result.flatten():\n",
    "            if idx < len(self.items):\n",
    "                item = {k: v for k, v in self.items[idx].items() if k != 'embedding'}\n",
    "                matched_items.append(item)\n",
    "        \n",
    "        # Generate response using Nova\n",
    "        response = self._invoke_nova_multimodal(query, matched_items)\n",
    "        \n",
    "        print(f\"✓ Found {len(matched_items)} relevant items\")\n",
    "        return response\n",
    "    \n",
    "    # def _invoke_nova_multimodal(self, prompt: str, matched_items: List[Dict]) -> str:\n",
    "    #     context = \"\\n\".join([f\"[Page {item['page']}] {item['text']}\" for item in matched_items if item['type'] in ['text', 'table']])\n",
    "        \n",
    "    #     try:\n",
    "    #         result = self.qa_pipeline(question=prompt, context=context[:2000])  # Limit context length\n",
    "    #         return f\"{result['answer']} (confidence: {result['score']:.2f})\"\n",
    "    #     except:\n",
    "    #         return f\"Based on the documents:\\n{context[:500]}...\"\n",
    "        \n",
    "    def _invoke_nova_multimodal(self, prompt: str, matched_items: List[Dict]) -> str:\n",
    "        # Build context from matched items\n",
    "        context_parts = []\n",
    "        \n",
    "        for item in matched_items:\n",
    "            if item['type'] in ['text', 'table', 'financial_text']:\n",
    "                context_parts.append(f\"Page {item['page']}: {item['text']}\")\n",
    "        \n",
    "        full_context = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # First try to find direct matches\n",
    "        prompt_lower = prompt.lower()\n",
    "        relevant_lines = []\n",
    "        \n",
    "        for line in full_context.split('\\n'):\n",
    "            line_lower = line.lower()\n",
    "            if any(term in line_lower for term in prompt_lower.split()):\n",
    "                relevant_lines.append(line.strip())\n",
    "        \n",
    "        # Use most relevant context for generation\n",
    "        if relevant_lines:\n",
    "            context_for_generation = \"\\n\".join(relevant_lines[:20])  # Top 20 relevant lines\n",
    "        else:\n",
    "            context_for_generation = full_context[:2000]  # First 2000 chars if no specific matches\n",
    "        \n",
    "        # Generate answer using T5\n",
    "        input_text = f\"Answer the question based on the context.\\n\\nContext: {context_for_generation}\\n\\nQuestion: {prompt}\\n\\nAnswer:\"\n",
    "        \n",
    "        try:\n",
    "            response = self.generator(input_text, max_length=150, min_length=10, do_sample=False)\n",
    "            generated_answer = response[0]['generated_text']\n",
    "            \n",
    "            # Add source information\n",
    "            return f\"{generated_answer}\\n\\n[Source: Found in {len(matched_items)} sections, most relevant from pages: {', '.join(str(item['page']) for item in matched_items[:3])}]\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Fallback to simple extraction\n",
    "            return f\"Based on the documents:\\n\\n\" + \"\\n\".join(relevant_lines[:5]) + f\"\\n\\n[Found in pages: {', '.join(str(item['page']) for item in matched_items[:3])}]\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the financial PDF processor\"\"\"\n",
    "    processor = FinancialPDFProcessor()\n",
    "    \n",
    "    print(\"=== Financial PDF Data Extractor with RAG ===\")\n",
    "    print(\"Specialized for precise financial data extraction from PDFs\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n=== Main Menu ===\")\n",
    "        print(\"1. Process new PDF\")\n",
    "        print(\"2. Query current PDF\")\n",
    "        print(\"3. Exit\")\n",
    "        \n",
    "        choice = input(\"Choose option (1-3): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            try:\n",
    "                filepath, source_type = processor.select_pdf_source()\n",
    "                result = processor.process_pdf(filepath)\n",
    "                print(f\"\\n✓ Successfully processed: {result['filepath']}\")\n",
    "                print(f\"  - {result['items_count']} items extracted\")\n",
    "                print(f\"  - {result['pages']} pages processed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error processing PDF: {e}\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            if not processor.items:\n",
    "                print(\"No PDF processed yet. Please process a PDF first.\")\n",
    "                continue\n",
    "            \n",
    "            query = input(\"\\nEnter your question: \").strip()\n",
    "            if query:\n",
    "                response = processor.query_documents(query)\n",
    "                print(f\"\\n=== Response ===\\n{response}\")\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7cb499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
